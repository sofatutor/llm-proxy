{{- $managementTokenSecret := include "llm-proxy.managementTokenSecretName" . }}
{{- $encryptionKeySecret := include "llm-proxy.encryptionKeySecretName" . }}
{{- $dbDriver := .Values.env.DB_DRIVER | default "sqlite" }}
{{- $postgresqlEnabled := .Values.postgresql.enabled }}
{{- if not $managementTokenSecret }}
********************************************************************************
****                            WARNING                                     ****
********************************************************************************

MANAGEMENT_TOKEN is REQUIRED but not configured!

The application will fail to start without MANAGEMENT_TOKEN.

Please configure one of the following:

1. Reference an existing Kubernetes Secret (RECOMMENDED):
   helm upgrade {{ .Release.Name }} {{ .Chart.Name }} \
     --set secrets.managementToken.existingSecret.name=your-secret-name

2. Use chart-managed secret (DEVELOPMENT ONLY):
   helm upgrade {{ .Release.Name }} {{ .Chart.Name }} \
     --set secrets.create=true \
     --set-string secrets.data.managementToken="your-token"

For more information, see the README.md in the chart directory.

********************************************************************************
{{- end }}

{{- if not $encryptionKeySecret }}
********************************************************************************
****                       SECURITY WARNING                                 ****
********************************************************************************

ENCRYPTION_KEY is not configured!

API keys and tokens will be stored in PLAINTEXT in the database.

For production deployments, strongly consider enabling encryption:

1. Create a secret with an encryption key:
   kubectl create secret generic llm-proxy-encryption \
     --from-literal=ENCRYPTION_KEY=$(openssl rand -base64 32)

2. Configure the chart to use it:
   helm upgrade {{ .Release.Name }} {{ .Chart.Name }} \
     --set secrets.encryptionKey.existingSecret.name=llm-proxy-encryption

For more information, see the README.md in the chart directory.

********************************************************************************
{{- end }}

{{- if and (eq $dbDriver "postgres") $postgresqlEnabled }}
********************************************************************************
****                   IN-CLUSTER POSTGRESQL ENABLED                       ****
********************************************************************************

You are using in-cluster PostgreSQL for development/testing.

Database Configuration:
  Host:     {{ include "llm-proxy.postgresql.host" . }}
  Port:     {{ include "llm-proxy.postgresql.port" . }}
  Database: {{ include "llm-proxy.postgresql.database" . }}
  Username: {{ include "llm-proxy.postgresql.username" . }}

IMPORTANT:
- Ensure your Docker image is built with PostgreSQL support (postgres build tag)
- Default images are built with: docker build --build-arg POSTGRES_SUPPORT=true
- In-cluster PostgreSQL is for development/testing only
- For production, use external PostgreSQL with secrets.databaseUrl.existingSecret

********************************************************************************
{{- end }}

{{- if .Values.dispatcher.enabled }}
********************************************************************************
****                      DISPATCHER DEPLOYMENT ENABLED                     ****
********************************************************************************

Dispatcher Configuration:
  Service:     {{ .Values.dispatcher.service }}
  Endpoint:    {{ include "llm-proxy.dispatcher.endpoint" . }}
  Replicas:    {{ .Values.dispatcher.replicaCount }}
  Event Bus:   {{ .Values.env.LLM_PROXY_EVENT_BUS }}
  {{- if and (eq .Values.dispatcher.service "file") .Values.dispatcher.persistence.enabled }}
  Persistence: Enabled ({{ .Values.dispatcher.persistence.size }})
  {{- end }}

The dispatcher is processing events from the event bus and forwarding them to {{ .Values.dispatcher.service }}.

To check dispatcher status:
  kubectl get pods --namespace {{ .Release.Namespace }} -l app.kubernetes.io/component=dispatcher
  kubectl logs --namespace {{ .Release.Namespace }} -l app.kubernetes.io/component=dispatcher

********************************************************************************
{{- end }}

1. Get the application URL:
{{- if .Values.ingress.enabled }}
{{- range .Values.ingress.hosts }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ .host }}
{{- end }}
{{- else if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "llm-proxy.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "llm-proxy.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "llm-proxy.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.service.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "llm-proxy.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}

{{- if and .Values.admin.enabled .Values.admin.ingress.enabled }}

Admin UI URL:
{{- range .Values.admin.ingress.hosts }}
  http{{ if $.Values.admin.ingress.tls }}s{{ end }}://{{ .host }}
{{- end }}
{{- end }}

2. Health check endpoint:
   http://<service-url>/health

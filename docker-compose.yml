x-llm-proxy-base: &llm-proxy-base
  image: llm-proxy:latest
  restart: unless-stopped
  entrypoint: ["/app/entrypoint.sh"]
  read_only: true
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  tmpfs:
    - /tmp
  environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY}
    - MANAGEMENT_TOKEN=${MANAGEMENT_TOKEN}
    - LOG_LEVEL=${LOG_LEVEL:-info}
    - ENABLE_METRICS=true
    - PORT=${PORT:-8080}
    - LLM_PROXY_EVENT_BUS=redis
    - REDIS_ADDR=redis:6379
  volumes:
    - ./tmp/llm-proxy-data:/app/data
    - ./tmp/llm-proxy-logs:/app/logs
    - ./tmp/llm-proxy-config:/app/config
    - .env:/app/.env
    - ./config/api_providers.yaml:/app/config/api_providers.yaml

services:
  llm-proxy:
    <<: *llm-proxy-base
    container_name: llm-proxy
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    command: ["server"]
    depends_on:
      - redis

  logger:
    <<: *llm-proxy-base
    container_name: llm-proxy-file-logger
    command: ["dispatcher", "--service", "file", "--endpoint", "/app/logs/events.jsonl"]
    volumes:
      - ./tmp/llm-proxy-logs:/app/logs
    depends_on:
      - redis

  helicone:
    <<: *llm-proxy-base
    container_name: llm-proxy-helicone-logger
    command: ["dispatcher", "--service", "helicone", "--api-key", "${HELICONE_API_KEY}"]
    depends_on:
      - redis

  admin:
    <<: *llm-proxy-base
    container_name: llm-proxy-admin
    command: ["admin"]
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost:8081/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    environment:
      - ADMIN_UI_API_BASE_URL=http://llm-proxy:8080
    depends_on:
      - llm-proxy

  redis:
    image: redis:8
    container_name: llm-proxy-redis
    ports:
      - "6379:6379"
    restart: unless-stopped
    command: ["redis-server", "--loglevel", "debug"]

volumes:
  llm-proxy-data:
    name: llm-proxy-data
  llm-proxy-logs:
    name: llm-proxy-logs
  llm-proxy-config:
    name: llm-proxy-config